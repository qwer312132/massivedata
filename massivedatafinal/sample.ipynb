{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import *\n",
    "import struct\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 打開數據，使用 decode 指定編碼格式 ASCII 進行解碼字符串的動作\n",
    "def unpackAfile(infile):\n",
    "    \n",
    "# == opening Afile ==\n",
    "    b= os.path.getsize(infile)\n",
    "    FH = open(infile, 'rb')\n",
    "    line = FH.read(b)\n",
    "    fileHeader= struct.unpack(\"<4s3h6bh6s\", line[0:24])\n",
    "    \n",
    "    fileLength = fileHeader[3]\n",
    "    port = fileHeader[10]\n",
    "    FirstStn = fileHeader[11][0:4].decode('ASCII').rstrip()\n",
    "    #print(fileHeader)\n",
    "# =================================Header===================================\n",
    "    \n",
    "    portHeader = []\n",
    "    for i in range(24,port*32,32):\n",
    "        port_data = struct.unpack(\"<4s4s3sbh2b4s12b\",line[i:i+32])\n",
    "        portHeader.append(port_data)\n",
    "\n",
    "# =================================Data===================================    \n",
    "\n",
    "    dataStartByte = 24+int(port)*32\n",
    "    dataPoint = 3*int(port)*int(fileLength)*100\n",
    "    times = int(port)*3*4\n",
    "    data=[]\n",
    "\n",
    "    data = struct.unpack(\"<%di\"%dataPoint,line[dataStartByte:dataStartByte+dataPoint*4])\n",
    "\n",
    "    \n",
    "    portHeader = np.array(portHeader)    \n",
    "    data = np.array(data)    \n",
    "    idata =data.reshape((3,port,fileLength*100),order='F')\n",
    "    \n",
    "#== write to obspy Stream --\n",
    "    #print(fileHeader)\n",
    "    #print(len(idata[0][0]))\n",
    "    sttime = UTCDateTime(fileHeader[1],fileHeader[4],fileHeader[5],fileHeader[6],fileHeader[7],fileHeader[8],fileHeader[2])\n",
    "    npts = fileHeader[3]*fileHeader[9]\n",
    "    samp = fileHeader[9]\n",
    "    #print(sttime)\n",
    "    # afst = Afile's Stream\n",
    "    afst = Stream()\n",
    "    \n",
    "    for stc in range(fileHeader[10]):\n",
    "        stn = portHeader[stc][0].decode('ASCII').rstrip()\n",
    "        instrument = portHeader[stc][1].decode('ASCII').rstrip()\n",
    "        loc = '0'+str(portHeader[stc][6].decode('ASCII'))\n",
    "        #net = \"TW\"\n",
    "        net = str(portHeader[stc][7].decode('ASCII')).rstrip()\n",
    "        GPS = int(portHeader[stc][3])\n",
    "        \n",
    "        # remove GPS unlock or broken station\n",
    "        if ( GPS == 1 or GPS == 2 ):\n",
    "            chc = 0\n",
    "            if instrument == 'FBA':\n",
    "                chc = 1\n",
    "            elif instrument == 'SP':\n",
    "                chc = 4\n",
    "            elif instrument == 'BB':\n",
    "                chc = 7\n",
    "            \n",
    "            #print(chc,instrument)\n",
    "            \n",
    "            # for each channel in port\n",
    "            for ch in range(3):\n",
    "                #print(num,ch,chc)\n",
    "                chn = 'Ch'+str(chc+ch)\n",
    "                #print(stc,channel)\n",
    "                \n",
    "                stats = {'network': net, 'station': stn, 'location': loc,\n",
    "                        'channel': chn, 'npts': npts, 'sampling_rate': samp,\n",
    "                        'starttime': sttime}\n",
    "                \n",
    "                data = np.array(idata[ch][stc], dtype=float)\n",
    "                sttmp = Stream([Trace(data=data, header=stats)])\n",
    "                afst += sttmp\n",
    "    \n",
    "    #stst = stms[0]\n",
    "    #stst.detrend('linear').plot()\n",
    "    #stms.write('test.mseed', format='MSEED')    \n",
    "    \n",
    "    return afst, FirstStn, fileHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import time\n",
    "\n",
    "\n",
    "weather = dict()\n",
    "weather1 = dict()\n",
    "\n",
    "# 創造地震資料 共20萬筆數據\n",
    "with open ('network20.csv', 'w', newline = '',encoding='big5') as csvfile:\n",
    "    datanames=['network','station','location','channel','npts','sampling_rate','starttime','delta','calib']\n",
    "    writer = csv.DictWriter(csvfile , fieldnames = datanames)\n",
    "    #writer.writeheader()\n",
    "    for j in range(0,5556):\n",
    "        for i in range(0,36):\n",
    "            net=str(unpackAfile('05290646.40C')[0][i].stats['network'])\n",
    "            stn=str(unpackAfile('05290646.40C')[0][i].stats['station'])\n",
    "            loc=str(unpackAfile('05290646.40C')[0][i].stats['location'])\n",
    "            chn=str(unpackAfile('05290646.40C')[0][i].stats['channel'])\n",
    "            npts=str(unpackAfile('05290646.40C')[0][i].stats['npts'])\n",
    "            samp=str(unpackAfile('05290646.40C')[0][i].stats['sampling_rate'])\n",
    "            sttime=str(unpackAfile('05290646.40C')[0][i].stats['starttime'])\n",
    "            delt=str(unpackAfile('05290646.40C')[0][i].stats['delta'])\n",
    "            cal=str(unpackAfile('05290646.40C')[0][i].stats['calib'])\n",
    "            weather['network']=net\n",
    "            weather['station']=stn\n",
    "            weather['location']=loc\n",
    "            weather['channel']=chn\n",
    "            weather['npts']=npts\n",
    "            weather['sampling_rate']=samp\n",
    "            weather['starttime']=sttime\n",
    "            weather['delta']=delt\n",
    "            weather['calib']=cal\n",
    "            writer.writerow(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創造五萬筆數據\n",
    "with open(\"network5.csv\", \"w\") as writer:\n",
    "    reader = open(\"network20.csv\", \"r\")\n",
    "    for _ in range(50000):\n",
    "        writer.write(reader.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算執行所花費的時間\n",
    "\"\"\"\n",
    "import time\n",
    "start1=time.time()\n",
    "\n",
    "某段的hive執行程式\n",
    "\n",
    "end=time.time()\n",
    "end=end-start1\n",
    "print(round(end,5))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive  # or import hive\n",
    "cursor = hive.connect(host='CT100',port=10000,auth='LDAP',username='root',password='00000', database='default').cursor()\n",
    "# connect hiveserver2\n",
    "# host為ubuntu主機名或ip address\n",
    "# username用建立/user/hive/warehouse的授權名,要不然會出現一些授權上的問題\n",
    "# database為資料庫\n",
    "# password是你在執行beeline時所輸入的密碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "請優先照 PyHive使用分享的PPT 執行 yarn-untils.py 了解你目前需要修改的資源配置，再到mapred-site.xml 和 yarn-site.xml 進行修改 (不修改可能導致程式執行資源不足)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#參考資料hive指令手冊網址https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\n",
    "cursor.execute('show tables')\n",
    "print (cursor.fetchall())\n",
    "# 顯示 table 有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE TABLE earthquakedata1 (network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n",
    "cursor.execute('show tables')\n",
    "print (cursor.fetchall())\n",
    "# 創建 table 地震資料的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "有 Partition 與 Bucket 的創建 table 方式請參考 PyHive使用分享的PPT 自行撰寫並執行\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition\n",
    "cursor.execute(\"CREATE TABLE earthquakedata2(network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) partitioned by (count INT) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n",
    "cursor.execute('show tables')\n",
    "print (cursor.fetchall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket\n",
    "cursor.execute(\"CREATE TABLE earthquakedata3(network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) partitioned by (count INT) CLUSTERED BY(station) SORTED BY(station) INTO 11 BUCKETS row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n",
    "cursor.execute('show tables')\n",
    "print (cursor.fetchall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('drop Table earthquakedata2')\n",
    "cursor.execute('show tables')\n",
    "print (cursor.fetchall())\n",
    "# 刪除 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"load data local inpath '/root/network5.csv' into table earthquakedata1\")\n",
    "# load data inpath 是用在ubuntu主機中的資料放入在earthquake1 table 中 , 而 ''是放路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊有兩種 insert table 的方式，請同學以自己目前要測的內容取消註解 \"#\" 來執行 hive 指令\n",
    "import time\n",
    "start = time.time()\n",
    "# 一般 table 插 一般 table\n",
    "#cursor.execute(\"insert into table earthquakedata2 select * from earthquakedata1\")\n",
    "\n",
    "# 一般 table 插 Partition table\n",
    "#cursor.execute(\"insert into table earthquakedata2 partition (count=1) select network , station , location , channel , npts , sampling_rate , starttime , delta , calib from earthquakedata1\")\n",
    "\n",
    "# 插入 earthquakedata1 到 earthquakedata2\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata1\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata2\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "其餘需要測試的部分請參考 PyHive 使用分享的 PPT 自行撰寫並執行\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"load data local inpath '/root/network5.csv' into table earthquakedata2 PARTITION (count=1)\")\n",
    "# 加載5萬筆地震資料放入第二個table的 count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"set hive.enforce.bucketing = true\")\n",
    "# 命令強制執行 bucketing\n",
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"from earthquakedata2 insert overwrite table earthquakedata3 PARTITION (count=1) select network , station , location , channel , npts , sampling_rate , starttime , delta , calib WHERE count=1\")\n",
    "# 插入 earthquakedata2 到 earthquakedata3\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata3\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start\n",
    "#query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('drop Table earthquakedata1')\n",
    "cursor.execute('drop Table earthquakedata2')\n",
    "cursor.execute('drop Table earthquakedata3')\n",
    "cursor.execute(\"CREATE TABLE earthquakedata1 (network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n",
    "cursor.execute(\"CREATE TABLE earthquakedata2(network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) partitioned by (count INT) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n",
    "cursor.execute(\"CREATE TABLE earthquakedata3(network STRING, station STRING, location STRING, channel STRING, npts STRING, sampling_rate STRING, starttime STRING, delta STRING, calib STRING) partitioned by (count INT) CLUSTERED BY(station) SORTED BY(station) INTO 11 BUCKETS row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"load data local inpath '/root/network20.csv' into table earthquakedata1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata1\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊有兩種 insert table 的方式，請同學以自己目前要測的內容取消註解 \"#\" 來執行 hive 指令\n",
    "import time\n",
    "start = time.time()\n",
    "# 一般 table 插 一般 table\n",
    "#cursor.execute(\"insert into table earthquakedata2 select * from earthquakedata1\")\n",
    "\n",
    "# 一般 table 插 Partition table\n",
    "#cursor.execute(\"insert into table earthquakedata2 partition (count=1) select network , station , location , channel , npts , sampling_rate , starttime , delta , calib from earthquakedata1\")\n",
    "\n",
    "# 插入 earthquakedata1 到 earthquakedata2\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata2\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"set hive.enforce.bucketing = true\")\n",
    "# 命令強制執行 bucketing\n",
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"from earthquakedata2 insert overwrite table earthquakedata3 PARTITION (count=1) select network , station , location , channel , npts , sampling_rate , starttime , delta , calib WHERE count=1\")\n",
    "# 插入 earthquakedata2 到 earthquakedata3\n",
    "end=time.time()\n",
    "end=end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "cursor.execute(\"select * from earthquakedata3\")\n",
    "print (cursor.fetchall())\n",
    "end=time.time()\n",
    "end=end-start\n",
    "#query 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
